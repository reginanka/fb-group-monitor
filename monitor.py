import requests
from bs4 import BeautifulSoup
import os
import re
from datetime import datetime, timedelta
from supabase import create_client, Client
import time

# Supabase
SUPABASE_URL = os.getenv('SUPABASE_URL')
SUPABASE_KEY = os.getenv('SUPABASE_KEY')
supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)

# Telegram
TG_BOT_TOKEN = os.getenv('TG_BOT_TOKEN')
TG_CHAT_ID = os.getenv('TG_CHAT_ID')

# Facebook –≥—Ä—É–ø–∞
GROUP_ID = os.getenv('GROUP_ID')
GROUP_URL = f"https://mbasic.facebook.com/groups/{GROUP_ID}"

def get_last_checkpoint():
    """–û—Ç—Ä–∏–º—É—î timestamp –æ—Å—Ç–∞–Ω–Ω—å–æ—ó –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏"""
    result = supabase.table('monitor_state')\
        .select('last_check_time')\
        .eq('group_id', GROUP_ID)\
        .execute()
    
    if result.data:
        return datetime.fromisoformat(result.data[0]['last_check_time'])
    else:
        # –ü–µ—Ä—à–∏–π –∑–∞–ø—É—Å–∫ ‚Äî –±–µ—Ä–µ–º–æ 3 –¥–Ω—ñ –Ω–∞–∑–∞–¥ –¥–ª—è —Ç–µ—Å—Ç—É
        return datetime.now() - timedelta(days=3)

def update_checkpoint():
    """–û–Ω–æ–≤–ª—é—î timestamp –æ—Å—Ç–∞–Ω–Ω—å–æ—ó –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏"""
    now = datetime.now().isoformat()
    supabase.table('monitor_state').upsert({
        'group_id': GROUP_ID,
        'last_check_time': now
    }).execute()

def scrape_facebook_posts(since_time):
    """
    –ü–∞—Ä—Å–∏—Ç—å –ø–æ—Å—Ç–∏ –∑ mbasic.facebook.com
    since_time: datetime ‚Äî –∑ —è–∫–æ–≥–æ —á–∞—Å—É —á–∏—Ç–∞—Ç–∏
    """
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept-Language': 'uk-UA,uk;q=0.9,en-US;q=0.8,en;q=0.7',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8'
    }
    
    posts = []
    url = GROUP_URL
    max_pages = 3
    
    for page in range(max_pages):
        try:
            print(f"üîç –ó–∞–≤–∞–Ω—Ç–∞–∂—É—é —Å—Ç–æ—Ä—ñ–Ω–∫—É {page + 1}: {url[:80]}...")
            response = requests.get(url, headers=headers, timeout=15)
            
            print(f"üì° –°—Ç–∞—Ç—É—Å: {response.status_code}, –†–æ–∑–º—ñ—Ä: {len(response.text)} –±–∞–π—Ç")
            
            if response.status_code != 200:
                print(f"‚ùå FB –ø–æ–≤–µ—Ä–Ω—É–≤ {response.status_code}, –∑—É–ø–∏–Ω—è—î–º–æ—Å—å")
                break
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # –î–µ–±–∞–≥: —à—É–∫–∞—î–º–æ —Ä—ñ–∑–Ω—ñ —Ç–∏–ø–∏ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä—ñ–≤
            post_divs = soup.find_all('div', {'data-ft': True})
            article_tags = soup.find_all('article')
            
            print(f"üì¶ –ó–Ω–∞–π–¥–µ–Ω–æ: {len(post_divs)} divs[data-ft], {len(article_tags)} articles")
            
            if not post_divs and not article_tags:
                print("‚ö†Ô∏è –ù–µ –∑–Ω–∞–π–¥–µ–Ω–æ –ø–æ—Å—Ç—ñ–≤. –ú–æ–∂–ª–∏–≤–æ –ø–æ—Ç—Ä—ñ–±–Ω–∞ –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü—ñ—è –∞–±–æ —Å–µ–ª–µ–∫—Ç–æ—Ä–∏ –∑–º—ñ–Ω–∏–ª–∏—Å—å")
                # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ HTML –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É
                with open('fb_debug.html', 'w', encoding='utf-8') as f:
                    f.write(response.text[:5000])
                print("üíæ –ü–µ—Ä—à—ñ 5000 —Å–∏–º–≤–æ–ª—ñ–≤ –∑–±–µ—Ä–µ–∂–µ–Ω–æ —É fb_debug.html")
                break
            
            found_old_post = False
            
            for post_div in post_divs:
                # –í–∏—Ç—è–≥—É—î–º–æ —á–∞—Å –ø–æ—Å—Ç–∞
                time_elem = post_div.find('abbr')
                if not time_elem:
                    continue
                
                post_time_str = time_elem.get_text()
                post_time = parse_fb_time(post_time_str)
                
                print(f"‚è∞ –ü–æ—Å—Ç: {post_time_str} -> {post_time}")
                
                if post_time < since_time:
                    found_old_post = True
                    print(f"‚èπÔ∏è –ó–Ω–∞–π—à–ª–∏ —Å—Ç–∞—Ä–∏–π –ø–æ—Å—Ç, –∑—É–ø–∏–Ω—è—î–º–æ—Å—å")
                    break
                
                # –í–∏—Ç—è–≥—É—î–º–æ –∞–≤—Ç–æ—Ä–∞
                author_elem = post_div.find('h3')
                author = author_elem.get_text().strip() if author_elem else 'Unknown'
                
                # –í–∏—Ç—è–≥—É—î–º–æ —Ç–µ–∫—Å—Ç
                content_elem = post_div.find('div', {'data-ft': True})
                text = content_elem.get_text().strip() if content_elem else ''
                
                # –í–∏—Ç—è–≥—É—î–º–æ –ø–æ—Å–∏–ª–∞–Ω–Ω—è –Ω–∞ –ø–æ—Å—Ç
                link_elem = post_div.find('a', href=True)
                post_link = GROUP_URL
                if link_elem and 'href' in link_elem.attrs:
                    href = link_elem['href']
                    if href.startswith('/'):
                        post_link = 'https://mbasic.facebook.com' + href
                
                # –í–∏—Ç—è–≥—É—î–º–æ user_id
                user_id = extract_user_id(post_div) or author
                
                posts.append({
                    'user_id': user_id,
                    'user_name': author,
                    'text': text[:500],  # –û–±–º–µ–∂—É—î–º–æ –¥–æ–≤–∂–∏–Ω—É
                    'link': post_link,
                    'timestamp': post_time
                })
                
                print(f"‚úÖ –î–æ–¥–∞–Ω–æ –ø–æ—Å—Ç –≤—ñ–¥ {author}")
            
            if found_old_post:
                break
            
            # –ù–∞—Å—Ç—É–ø–Ω–∞ —Å—Ç–æ—Ä—ñ–Ω–∫–∞
            next_link = soup.find('a', string=re.compile('See more posts|–ü–æ–∫–∞–∑–∞—Ç–∏ –±—ñ–ª—å—à–µ|Show more'))
            if not next_link or not next_link.get('href'):
                print("üèÅ –ù–µ–º–∞—î –Ω–∞—Å—Ç—É–ø–Ω–æ—ó —Å—Ç–æ—Ä—ñ–Ω–∫–∏")
                break
            
            url = 'https://mbasic.facebook.com' + next_link['href']
            time.sleep(3)  # –ó–±—ñ–ª—å—à–µ–Ω–∞ –∑–∞—Ç—Ä–∏–º–∫–∞
            
        except Exception as e:
            print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥—É: {e}")
            import traceback
            traceback.print_exc()
            break
    
    return posts

def parse_fb_time(time_str):
    """
    –ü–∞—Ä—Å–∏—Ç—å —á–∞—Å –∑ Facebook
    """
    now = datetime.now()
    time_str_lower = time_str.lower()
    
    try:
        if 'min' in time_str_lower or '—Ö–≤' in time_str_lower:
            match = re.search(r'(\d+)', time_str)
            if match:
                mins = int(match.group(1))
                return now - timedelta(minutes=mins)
        elif 'hr' in time_str_lower or 'hour' in time_str_lower or '–≥–æ–¥' in time_str_lower:
            match = re.search(r'(\d+)', time_str)
            if match:
                hrs = int(match.group(1))
                return now - timedelta(hours=hrs)
        elif 'yesterday' in time_str_lower or '–≤—á–æ—Ä–∞' in time_str_lower:
            return now - timedelta(days=1)
        elif 'day' in time_str_lower or '–¥–Ω' in time_str_lower:
            match = re.search(r'(\d+)', time_str)
            if match:
                days = int(match.group(1))
                return now - timedelta(days=days)
    except:
        pass
    
    # –ó–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º - 1 –≥–æ–¥–∏–Ω–∞ –Ω–∞–∑–∞–¥
    return now - timedelta(hours=1)

def extract_user_id(post_div):
    """–í–∏—Ç—è–≥—É—î user_id –∑ –ø–æ—Å–∏–ª–∞–Ω–Ω—è –Ω–∞ –ø—Ä–æ—Ñ—ñ–ª—å"""
    profile_link = post_div.find('a', href=re.compile(r'/profile\.php\?id=|/[^/]+\?'))
    if profile_link:
        match = re.search(r'id=(\d+)', profile_link['href'])
        if match:
            return match.group(1)
    return None

def extract_first_sentence(text, limit=100):
    """–ü–µ—Ä—à–µ —Ä–µ—á–µ–Ω–Ω—è –∞–±–æ –ø–µ—Ä—à—ñ 100 —Å–∏–º–≤–æ–ª—ñ–≤"""
    if not text:
        return ""
    text = re.sub(r'<[^>]+>|https?://\S+', '', text)
    match = re.match(r'^[^.!?]+[.!?]', text)
    if match:
        return match.group(0)[:limit]
    return text[:limit].strip()

def check_spam_patterns(user_id, first_sentence):
    """–î–µ—Ç–µ–∫—Ü—ñ—è —Å–ø–∞–º—É"""
    cutoff = (datetime.now() - timedelta(hours=24)).isoformat()
    result = supabase.table('group_posts')\
        .select('first_sentence, created_at')\
        .eq('user_id', user_id)\
        .gte('created_at', cutoff)\
        .execute()
    
    posts = result.data
    
    if len(posts) > 4:
        similar = sum(1 for p in posts if p['first_sentence'] == first_sentence)
        if similar > 2:
            return True, len(posts), "–î—É–±–ª—ñ–∫–∞—Ç —Ç–µ–∫—Å—Ç—É"
        return True, len(posts), "–ë–∞–≥–∞—Ç–æ –ø–æ—Å—Ç—ñ–≤"
    
    return False, 0, ""

def send_telegram(message):
    """–ù–∞–¥—Å–∏–ª–∞—î –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è –≤ Telegram"""
    url = f"https://api.telegram.org/bot{TG_BOT_TOKEN}/sendMessage"
    try:
        requests.post(url, data={
            'chat_id': TG_CHAT_ID,
            'text': message,
            'parse_mode': 'HTML',
            'disable_web_page_preview': True
        }, timeout=5)
    except Exception as e:
        print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ Telegram: {e}")

def process_posts(posts):
    """–û–±—Ä–æ–±–∫–∞ —Ç–∞ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è –ø–æ—Å—Ç—ñ–≤"""
    new_count = 0
    spam_count = 0
    
    for post in posts:
        # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —á–∏ —î –≤ –ë–î
        existing = supabase.table('group_posts')\
            .select('id')\
            .eq('post_link', post['link'])\
            .execute()
        
        if existing.data:
            continue
        
        # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ
        first_sent = extract_first_sentence(post['text'])
        try:
            supabase.table('group_posts').insert({
                'user_id': post['user_id'],
                'user_name': post['user_name'],
                'post_link': post['link'],
                'first_sentence': first_sent,
                'created_at': post['timestamp'].isoformat()
            }).execute()
            new_count += 1
        except Exception as e:
            print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –∑–∞–ø–∏—Å—É –≤ –ë–î: {e}")
            continue
        
        # –î–µ—Ç–µ–∫—Ü—ñ—è —Å–ø–∞–º—É
        is_spam, count, reason = check_spam_patterns(post['user_id'], first_sent)
        
        if is_spam:
            spam_count += 1
            message = f"üö® –ü—ñ–¥–æ–∑—Ä—ñ–ª–∞ –∞–∫—Ç–∏–≤–Ω—ñ—Å—Ç—å\n\n"\
                     f"üë§ {post['user_name']} (ID: {post['user_id']})\n"\
                     f"üìä –ü–æ—Å—Ç—ñ–≤ –∑–∞ 24 –≥–æ–¥: {count}\n"\
                     f"‚ö†Ô∏è –ü—Ä–∏—á–∏–Ω–∞: {reason}\n"\
                     f"üìù –¢–µ–∫—Å—Ç: {first_sent}\n"\
                     f"üîó <a href='{post['link']}'>–ü–µ—Ä–µ–≥–ª—è–Ω—É—Ç–∏ –ø–æ—Å—Ç</a>"
            send_telegram(message)
    
    return new_count, spam_count

def main():
    print(f"üöÄ –ó–∞–ø—É—Å–∫ –º–æ–Ω—ñ—Ç–æ—Ä–∏–Ω–≥—É –≥—Ä—É–ø–∏ {GROUP_ID}")
    
    # –û—Ç—Ä–∏–º—É—î–º–æ –æ—Å—Ç–∞–Ω–Ω—ñ–π checkpoint
    last_check = get_last_checkpoint()
    print(f"üìÖ –ß–∏—Ç–∞—î–º–æ –ø–æ—Å—Ç–∏ –ø—ñ—Å–ª—è {last_check}")
    
    # –ü–∞—Ä—Å–∏–º–æ FB
    posts = scrape_facebook_posts(last_check)
    print(f"üìÑ –ó–Ω–∞–π–¥–µ–Ω–æ {len(posts)} –Ω–æ–≤–∏—Ö –ø–æ—Å—Ç—ñ–≤")
    
    # –û–±—Ä–æ–±–ª—è—î–º–æ
    new_count, spam_count = process_posts(posts)
    
    # –û–Ω–æ–≤–ª—é—î–º–æ checkpoint
    update_checkpoint()
    
    print(f"‚úÖ –û–±—Ä–æ–±–ª–µ–Ω–æ: {new_count} –Ω–æ–≤–∏—Ö, {spam_count} —Å–ø–∞–º–µ—Ä—ñ–≤")
    
    # –ü—ñ–¥—Å—É–º–∫–æ–≤–µ –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è
    if new_count > 0:
        summary = f"üìä –ú–æ–Ω—ñ—Ç–æ—Ä–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω–æ\n"\
                 f"–ù–æ–≤–∏—Ö –ø–æ—Å—Ç—ñ–≤: {new_count}\n"\
                 f"–°–ø–∞–º–µ—Ä—ñ–≤: {spam_count}"
        send_telegram(summary)

if __name__ == "__main__":
    main()
